{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument_data.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Script to create an augmented dataset.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "from imageio.v3 import imread, imwrite\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def Rotate(deg=20):\n",
    "    \"\"\"Return function to rotate image.\"\"\"\n",
    "\n",
    "    def _rotate(img):\n",
    "        \"\"\"Rotate a random integer amount in the range (-deg, deg) (inclusive).\n",
    "\n",
    "        Keep the dimensions the same and fill any missing pixels with black.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "        \"\"\"\n",
    "        # TODO: implement _rotate(img)\n",
    "        return rotate(\n",
    "            input=img, angle=np.random.randint(-deg, deg), reshape=False)\n",
    "        \n",
    "    return _rotate\n",
    "\n",
    "\n",
    "\n",
    "def Grayscale():\n",
    "    \"\"\"Return function to grayscale image.\"\"\"\n",
    "\n",
    "    def _grayscale(img):\n",
    "        \"\"\"Return 3-channel grayscale of image.\n",
    "\n",
    "        Compute grayscale values by taking average across the three channels.\n",
    "\n",
    "        Round to the nearest integer.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: implement _grayscale(img)\n",
    "        grayscale_img = np.mean(img, axis=2)\n",
    "        grayscale_img = np.round(grayscale_img).astype(np.uint8)\n",
    "        grayscale_img = np.stack([grayscale_img] * 3, axis=-1)\n",
    "        return grayscale_img\n",
    "\n",
    "    return _grayscale\n",
    "\n",
    "\n",
    "def augment(filename, transforms, n=1, original=True):\n",
    "    \"\"\"Augment image at filename.\n",
    "\n",
    "    :filename: name of image to be augmented\n",
    "    :transforms: List of image transformations\n",
    "    :n: number of augmented images to save\n",
    "    :original: whether to include the original images in the augmented dataset or not\n",
    "    :returns: a list of augmented images, where the first image is the original\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Augmenting {filename}\")\n",
    "    img = imread(filename)\n",
    "    res = [img] if original else []\n",
    "    for i in range(n):\n",
    "        new = img\n",
    "        for transform in transforms:\n",
    "            new = transform(new)\n",
    "        res.append(new)\n",
    "    return res\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Create augmented dataset.\"\"\"\n",
    "    reader = csv.DictReader(open(args.input, \"r\"), delimiter=\",\")\n",
    "    writer = csv.DictWriter(\n",
    "        open(f\"{args.datadir}/augmented_landmarks.csv\", \"w\"),\n",
    "        fieldnames=[\"filename\", \"semantic_label\", \"partition\", \"numeric_label\", \"task\"],\n",
    "    )\n",
    "    augment_partitions = set(args.partitions)\n",
    "\n",
    "    # TODO: change `augmentations` to specify which augmentations to apply\n",
    "    # augmentations = [Grayscale(), Rotate()]\n",
    "    augmentations = [Grayscale()]\n",
    "    # augmentations = [Rotate()]\n",
    "\n",
    "    writer.writeheader()\n",
    "    os.makedirs(f\"{args.datadir}/augmented/\", exist_ok=True)\n",
    "    for f in glob.glob(f\"{args.datadir}/augmented/*\"):\n",
    "        print(f\"Deleting {f}\")\n",
    "        os.remove(f)\n",
    "    for row in reader:\n",
    "        if row[\"partition\"] not in augment_partitions:\n",
    "            imwrite(\n",
    "                f\"{args.datadir}/augmented/{row['filename']}\",\n",
    "                imread(f\"{args.datadir}/images/{row['filename']}\"),\n",
    "            )\n",
    "            writer.writerow(row)\n",
    "            continue\n",
    "        imgs = augment(\n",
    "            f\"{args.datadir}/images/{row['filename']}\",\n",
    "            augmentations,\n",
    "            n=1,\n",
    "            original=False,  # TODO: change to False to exclude original image.\n",
    "        )\n",
    "        for i, img in enumerate(imgs):\n",
    "            fname = f\"{row['filename'][:-4]}_aug_{i}.png\"\n",
    "            imwrite(f\"{args.datadir}/augmented/{fname}\", img)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"filename\": fname,\n",
    "                    \"semantic_label\": row[\"semantic_label\"],\n",
    "                    \"partition\": row[\"partition\"],\n",
    "                    \"numeric_label\": row[\"numeric_label\"],\n",
    "                    \"task\": row[\"task\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"datadir\", help=\"Data directory\", default=\"./data/\")\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--partitions\",\n",
    "        nargs=\"+\",\n",
    "        help=\"Partitions \\\n",
    "            (train|val|test|challenge|none)+ \\\n",
    "                to apply augmentations to. Defaults to train\",\n",
    "        default=[\"train\"],\n",
    "    )\n",
    "    main(parser.parse_args(sys.argv[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge_augment_data.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Script to create an augmented dataset.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import torchvision.transforms as tortransformers\n",
    "from imageio.v3 import imread, imwrite\n",
    "\n",
    "import cv2\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def Rotate(deg=20):\n",
    "    \"\"\"Return function to rotate image.\"\"\"\n",
    "\n",
    "    def _rotate(img):\n",
    "        \"\"\"Rotate a random integer amount in the range (-deg, deg) (inclusive).\n",
    "\n",
    "        Keep the dimensions the same and fill any missing pixels with black.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "        \"\"\"\n",
    "        # TODO: implement _rotate(img)\n",
    "        return rotate(\n",
    "            input=img, angle=np.random.randint(-deg, deg), reshape=False)\n",
    "        \n",
    "    return _rotate\n",
    "\n",
    "\n",
    "\n",
    "def Grayscale():\n",
    "    \"\"\"Return function to grayscale image.\"\"\"\n",
    "\n",
    "    def _grayscale(img):\n",
    "        \"\"\"Return 3-channel grayscale of image.\n",
    "\n",
    "        Compute grayscale values by taking average across the three channels.\n",
    "\n",
    "        Round to the nearest integer.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: implement _grayscale(img)\n",
    "        grayscale_img = np.mean(img, axis=2)\n",
    "        grayscale_img = np.round(grayscale_img).astype(np.uint8)\n",
    "        grayscale_img = np.stack([grayscale_img] * 3, axis=-1)\n",
    "        return grayscale_img\n",
    "\n",
    "    return _grayscale\n",
    "\n",
    "\n",
    "def augment(filename, transforms, n=1, original=True):\n",
    "    \"\"\"Augment image at filename.\n",
    "\n",
    "    :filename: name of image to be augmented\n",
    "    :transforms: List of image transformations\n",
    "    :n: number of augmented images to save\n",
    "    :original: whether to include the original images in the augmented dataset or not\n",
    "    :returns: a list of augmented images, where the first image is the original\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Augmenting {filename}\")\n",
    "    img = imread(filename)\n",
    "    res = [img] if original else []\n",
    "    for i in range(n):\n",
    "        new = img\n",
    "        for transform in transforms:\n",
    "            new = transform(new)\n",
    "        res.append(new)\n",
    "    return res\n",
    "\n",
    "transform_shape = tortransformers.Compose([\n",
    "    tortransformers.RandomRotation(degrees=(-7, 7)),\n",
    "    # tortransformers.RandomResizedCrop(\n",
    "    #     (64, 64), scale=(0.7, 1), ratio=(0.5, 2)),\n",
    "    tortransformers.RandomHorizontalFlip(),\n",
    "    # tortransformers.ToTensor(),\n",
    "])\n",
    "\n",
    "transformer_color = tortransformers.ColorJitter(\n",
    "  brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5\n",
    ")\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Create augmented dataset.\"\"\"\n",
    "    \n",
    "    use_feature_selection = input(\"do you want to use feature selection? y/n\\n\")\n",
    "    \n",
    "    reader = csv.DictReader(open(args.input, \"r\"), delimiter=\",\")\n",
    "    writer = csv.DictWriter(\n",
    "        open(f\"{args.datadir}/augmented_landmarks.csv\", \"w\"),\n",
    "        fieldnames=\n",
    "            [\"filename\", \"semantic_label\", \"partition\", \"numeric_label\", \"task\"],\n",
    "    )\n",
    "    augment_partitions = set(args.partitions)\n",
    "\n",
    "    # TODO: change `augmentations` to specify which augmentations to apply\n",
    "    # augmentations = [Grayscale(), Rotate()]\n",
    "    # augmentations = [Grayscale(), tortransformers.ToPILImage(), transformer_color]\n",
    "    augmentations = [tortransformers.ToPILImage(), transform_shape, transformer_color]\n",
    "    # augmentations = [Rotate()]\n",
    "\n",
    "    writer.writeheader()\n",
    "    os.makedirs(f\"{args.datadir}/challenge_augmented/\", exist_ok=True)\n",
    "    for f in glob.glob(f\"{args.datadir}/challenge_augmented/*\"):\n",
    "        print(f\"Deleting {f}\")\n",
    "        os.remove(f)\n",
    "    for row in reader:\n",
    "        if row[\"partition\"] not in augment_partitions:\n",
    "            imwrite(\n",
    "                f\"{args.datadir}/challenge_augmented/{row['filename']}\",\n",
    "                imread(f\"{args.datadir}/images/{row['filename']}\"),\n",
    "            )\n",
    "            writer.writerow(row)\n",
    "            continue\n",
    "        imgs = augment(\n",
    "            f\"{args.datadir}/images/{row['filename']}\",\n",
    "            augmentations,\n",
    "            n=1,\n",
    "            original=True,  # TODO: change to False to exclude original image.\n",
    "        )\n",
    "        for i, img in enumerate(imgs):\n",
    "            fname = f\"{row['filename'][:-4]}_aug_{i}.png\"\n",
    "          \n",
    "            imwrite(f\"{args.datadir}/challenge_augmented/{fname}\", img)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"filename\": fname,\n",
    "                    \"semantic_label\": row[\"semantic_label\"],\n",
    "                    \"partition\": row[\"partition\"],\n",
    "                    \"numeric_label\": row[\"numeric_label\"],\n",
    "                    \"task\": row[\"task\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"datadir\", help=\"Data directory\", default=\"./data/\")\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--partitions\",\n",
    "        nargs=\"+\",\n",
    "        help=\"Partitions \\\n",
    "            (train|val|test|challenge|none)+ \\\n",
    "                to apply augmentations to. Defaults to train\",\n",
    "        default=[\"train\"],\n",
    "    )\n",
    "    main(parser.parse_args(sys.argv[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge_target.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Script to create an augmented dataset.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import torchvision.transforms as tortransformers\n",
    "from imageio.v3 import imread, imwrite\n",
    "\n",
    "import cv2\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def Rotate(deg=20):\n",
    "    \"\"\"Return function to rotate image.\"\"\"\n",
    "\n",
    "    def _rotate(img):\n",
    "        \"\"\"Rotate a random integer amount in the range (-deg, deg) (inclusive).\n",
    "\n",
    "        Keep the dimensions the same and fill any missing pixels with black.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "        \"\"\"\n",
    "        # TODO: implement _rotate(img)\n",
    "        return rotate(\n",
    "            input=img, angle=np.random.randint(-deg, deg), reshape=False)\n",
    "        \n",
    "    return _rotate\n",
    "\n",
    "\n",
    "\n",
    "def Grayscale():\n",
    "    \"\"\"Return function to grayscale image.\"\"\"\n",
    "\n",
    "    def _grayscale(img):\n",
    "        \"\"\"Return 3-channel grayscale of image.\n",
    "\n",
    "        Compute grayscale values by taking average across the three channels.\n",
    "\n",
    "        Round to the nearest integer.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: implement _grayscale(img)\n",
    "        grayscale_img = np.mean(img, axis=2)\n",
    "        grayscale_img = np.round(grayscale_img).astype(np.uint8)\n",
    "        grayscale_img = np.stack([grayscale_img] * 3, axis=-1)\n",
    "        return grayscale_img\n",
    "\n",
    "    return _grayscale\n",
    "\n",
    "\n",
    "def augment(filename, transforms, n=1, original=True):\n",
    "    \"\"\"Augment image at filename.\n",
    "\n",
    "    :filename: name of image to be augmented\n",
    "    :transforms: List of image transformations\n",
    "    :n: number of augmented images to save\n",
    "    :original: whether to include the original images in the augmented dataset or not\n",
    "    :returns: a list of augmented images, where the first image is the original\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Augmenting {filename}\")\n",
    "    img = imread(filename)\n",
    "    res = [img] if original else []\n",
    "    for i in range(n):\n",
    "        new = img\n",
    "        for transform in transforms:\n",
    "            new = transform(new)\n",
    "        res.append(new)\n",
    "    return res\n",
    "\n",
    "transform_shape = tortransformers.Compose([\n",
    "    tortransformers.RandomRotation(degrees=(-7, 7)),\n",
    "    # tortransformers.RandomResizedCrop(\n",
    "    #     (64, 64), scale=(0.7, 1), ratio=(0.5, 2)),\n",
    "    tortransformers.RandomHorizontalFlip(),\n",
    "    # tortransformers.ToTensor(),\n",
    "])\n",
    "\n",
    "transformer_color = tortransformers.ColorJitter(\n",
    "  brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5\n",
    ")\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Create augmented dataset.\"\"\"\n",
    "    \n",
    "    use_feature_selection = input(\"do you want to use feature selection? y/n\\n\")\n",
    "    \n",
    "    reader = csv.DictReader(open(args.input, \"r\"), delimiter=\",\")\n",
    "    writer = csv.DictWriter(\n",
    "        open(f\"{args.datadir}/augmented_landmarks.csv\", \"w\"),\n",
    "        fieldnames=[\n",
    "            \"filename\", \"semantic_label\", \"partition\", \"numeric_label\", \"task\"],\n",
    "    )\n",
    "    augment_partitions = set(args.partitions)\n",
    "\n",
    "    # TODO: change `augmentations` to specify which augmentations to apply\n",
    "    # augmentations = [Grayscale(), Rotate()]\n",
    "    # augmentations = [Grayscale(), tortransformers.ToPILImage(), transformer_color]\n",
    "    augmentations = [tortransformers.ToPILImage(), transform_shape, transformer_color]\n",
    "    # augmentations = [Rotate()]\n",
    "\n",
    "    writer.writeheader()\n",
    "    os.makedirs(f\"{args.datadir}/challenge_augmented/\", exist_ok=True)\n",
    "    for f in glob.glob(f\"{args.datadir}/challenge_augmented/*\"):\n",
    "        print(f\"Deleting {f}\")\n",
    "        os.remove(f)\n",
    "    for row in reader:\n",
    "        if row[\"partition\"] not in augment_partitions:\n",
    "            imwrite(\n",
    "                f\"{args.datadir}/challenge_augmented/{row['filename']}\",\n",
    "                imread(f\"{args.datadir}/images/{row['filename']}\"),\n",
    "            )\n",
    "            writer.writerow(row)\n",
    "            continue\n",
    "        imgs = augment(\n",
    "            f\"{args.datadir}/images/{row['filename']}\",\n",
    "            augmentations,\n",
    "            n=1,\n",
    "            original=True,  # TODO: change to False to exclude original image.\n",
    "        )\n",
    "        for i, img in enumerate(imgs):\n",
    "            fname = f\"{row['filename'][:-4]}_aug_{i}.png\"\n",
    "          \n",
    "            imwrite(f\"{args.datadir}/challenge_augmented/{fname}\", img)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"filename\": fname,\n",
    "                    \"semantic_label\": row[\"semantic_label\"],\n",
    "                    \"partition\": row[\"partition\"],\n",
    "                    \"numeric_label\": row[\"numeric_label\"],\n",
    "                    \"task\": row[\"task\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"datadir\", help=\"Data directory\", default=\"./data/\")\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--partitions\",\n",
    "        nargs=\"+\",\n",
    "        help=\"Partitions \\\n",
    "            (train|val|test|challenge|none)+ \\\n",
    "                to apply augmentations to. Defaults to train\",\n",
    "        default=[\"train\"],\n",
    "    )\n",
    "    main(parser.parse_args(sys.argv[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Generate confusion matrix graphs.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.source import Source\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_labels(loader, model):\n",
    "    \"\"\"Return true and predicted values.\"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "    for X, y in loader:\n",
    "        with torch.no_grad():\n",
    "            output = model(X)\n",
    "            predicted = predictions(output.data)\n",
    "            y_true = np.append(y_true, y.numpy())\n",
    "            y_pred = np.append(y_pred, predicted.numpy())\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def plot_conf(loader, model, sem_labels, png_name):\n",
    "    \"\"\"Draw confusion matrix.\"\"\"\n",
    "    y_true, y_pred = gen_labels(loader, model)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues, interpolation=\"nearest\")\n",
    "    cbar = fig.colorbar(cax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Frequency\", rotation=270, labelpad=10)\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        ax.text(j, i, z, ha=\"center\", va=\"center\")\n",
    "    plt.gcf().text(0.02, 0.4, sem_labels, fontsize=9)\n",
    "    plt.subplots_adjust(left=0.5)\n",
    "    ax.set_xlabel(\"Predictions\")\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.set_ylabel(\"True Labels\")\n",
    "    plt.savefig(png_name)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Create confusion matrix and save to file.\"\"\"\n",
    "    tr_loader, va_loader, te_loader, semantic_labels = get_train_val_test_loaders(\n",
    "        task=\"source\", batch_size=config(\"source.batch_size\")\n",
    "    )\n",
    "\n",
    "    model = Source()\n",
    "    print(\"Loading source...\")\n",
    "    model, epoch, stats = restore_checkpoint(model, config(\"source.checkpoint\"))\n",
    "\n",
    "    sem_labels = \n",
    "        \"0 - Colosseum\\n1 - Petronas Towers\\n2 - Rialto Bridge\\n3 - Museu Nacional d'Art de Catalunya\\n4 - St Stephen's Cathedral in Vienna\\n5 - Berlin Cathedral\\n6 - Hagia Sophia\\n7 - Gaudi Casa Batllo in Barcelona\"\n",
    "\n",
    "    # Evaluate model\n",
    "    plot_conf(va_loader, model, sem_labels, \"conf_matrix.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_challenge.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Landmarks Dataset\n",
    "    Class wrapper for interfacing with the dataset of landmark images\n",
    "    Usage: python dataset.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio.v3 import imread\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import config\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def get_train_val_test_loaders(task, batch_size, **kwargs):\n",
    "    \"\"\"Return DataLoaders for train, val and test splits.\n",
    "\n",
    "    Any keyword arguments are forwarded to the LandmarksDataset constructor.\n",
    "    \"\"\"\n",
    "    tr, va, te, _ = get_train_val_test_datasets(task, **kwargs)\n",
    "\n",
    "    tr_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va, batch_size=batch_size, shuffle=False)\n",
    "    te_loader = DataLoader(te, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return tr_loader, va_loader, te_loader, tr.get_semantic_label\n",
    "\n",
    "\n",
    "def get_challenge(task, batch_size, **kwargs):\n",
    "    \"\"\"Return DataLoader for challenge dataset.\n",
    "\n",
    "    Any keyword arguments are forwarded to the LandmarksDataset constructor.\n",
    "    \"\"\"\n",
    "    tr = LandmarksDataset(\"train\", task, **kwargs)\n",
    "    ch = LandmarksDataset(\"challenge\", task, **kwargs)\n",
    "\n",
    "    standardizer = ImageStandardizer()\n",
    "    standardizer.fit(tr.X)\n",
    "    tr.X = standardizer.transform(tr.X)\n",
    "    ch.X = standardizer.transform(ch.X)\n",
    "\n",
    "    tr.X = tr.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    ch.X = ch.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    ch_loader = DataLoader(ch, batch_size=batch_size, shuffle=False)\n",
    "    return ch_loader, tr.get_semantic_label\n",
    "\n",
    "\n",
    "def get_train_val_test_datasets(task=\"default\", **kwargs):\n",
    "    \"\"\"Return LandmarksDatasets and image standardizer.\n",
    "\n",
    "    Image standardizer should be fit to train data and applied to all splits.\n",
    "    \"\"\"\n",
    "    tr = LandmarksDataset(\"train\", task, **kwargs)\n",
    "    va = LandmarksDataset(\"val\", task, **kwargs)\n",
    "    te = LandmarksDataset(\"test\", task, **kwargs)\n",
    "\n",
    "    # Resize\n",
    "    # You may want to experiment with resizing images to be smaller\n",
    "    # for the challenge portion. How might this affect your training?\n",
    "    # tr.X = resize(tr.X)\n",
    "    # va.X = resize(va.X)\n",
    "    # te.X = resize(te.X)\n",
    "\n",
    "    # Standardize\n",
    "    standardizer = ImageStandardizer()\n",
    "    standardizer.fit(tr.X)\n",
    "    tr.X = standardizer.transform(tr.X)\n",
    "    va.X = standardizer.transform(va.X)\n",
    "    te.X = standardizer.transform(te.X)\n",
    "\n",
    "    # Transpose the dimensions from (N,H,W,C) to (N,C,H,W)\n",
    "    tr.X = tr.X.transpose(0, 3, 1, 2)\n",
    "    va.X = va.X.transpose(0, 3, 1, 2)\n",
    "    te.X = te.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return tr, va, te, standardizer\n",
    "\n",
    "\n",
    "def resize(X):\n",
    "    \"\"\"Resize the data partition X to the size specified in the config file.\n",
    "\n",
    "    Use bicubic interpolation for resizing.\n",
    "\n",
    "    Returns:\n",
    "        the resized images as a numpy array.\n",
    "    \"\"\"\n",
    "    image_dim = config(\"image_dim\")\n",
    "    image_size = (image_dim, image_dim)\n",
    "    resized = []\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = Image.fromarray(X[i]).resize(image_size, resample=2)\n",
    "        resized.append(xi)\n",
    "    resized = [np.asarray(im) for im in resized]\n",
    "    resized = np.array(resized)\n",
    "\n",
    "    return resized\n",
    "\n",
    "\n",
    "class ImageStandardizer(object):\n",
    "    \"\"\"Standardize a batch of images to mean 0 and variance 1.\n",
    "\n",
    "    The standardization should be applied separately to each channel.\n",
    "    The mean and standard deviation parameters are computed in `fit(X)` and\n",
    "    applied using `transform(X)`.\n",
    "\n",
    "    X has shape (N, image_height, image_width, color_channel), where N is\n",
    "    the number of images in the set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mean and standard deviations to None.\"\"\"\n",
    "        super().__init__()\n",
    "        self.image_mean = None\n",
    "        self.image_std = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Calculate per-channel mean and standard deviation from dataset X.\n",
    "        Hint: you may find the axis parameter helpful\"\"\"\n",
    "        self.image_mean = np.mean(X, axis=(0,1,2))\n",
    "        self.image_std = np.std(X, axis=(0,1,2))\n",
    "            \n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Return standardized dataset given dataset X.\"\"\"\n",
    "        X1 = X - self.image_mean\n",
    "        X2 = X1 / self.image_std\n",
    "        return X2\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "class LandmarksDataset(Dataset):\n",
    "    \"\"\"Dataset class for landmark images.\"\"\"\n",
    "\n",
    "    def __init__(self, partition, task=\"target\", augment=False):\n",
    "        \"\"\"Read in the necessary data from disk.\n",
    "\n",
    "        For parts 2, 3 and data augmentation, `task` should be \"target\".\n",
    "        For source task of part 4, `task` should be \"source\".\n",
    "\n",
    "        For data augmentation, `augment` should be True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if partition not in [\"train\", \"val\", \"test\", \"challenge\"]:\n",
    "            raise ValueError(\"Partition {} does not exist\".format(partition))\n",
    "\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        random.seed(42)\n",
    "        self.partition = partition\n",
    "        self.task = task\n",
    "        self.augment = augment\n",
    "        # Load in all the data we need from disk\n",
    "        if task == \"target\" or task == \"source\":\n",
    "            self.metadata = pd.read_csv(config(\"csv_file\"))\n",
    "        if self.augment:\n",
    "            print(\"Augmented\")\n",
    "            self.metadata = pd.read_csv(config(\"augmented_csv_file\"))\n",
    "        self.X, self.y = self._load_data()\n",
    "\n",
    "        self.semantic_labels = dict(\n",
    "            zip(\n",
    "                self.metadata[self.metadata.task == self.task][\"numeric_label\"],\n",
    "                self.metadata[self.metadata.task == self.task][\"semantic_label\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return size of dataset.\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (image, label) pair at index `idx` of dataset.\"\"\"\n",
    "        return torch.from_numpy(\n",
    "            self.X[idx]).float(), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load a single data partition from file.\"\"\"\n",
    "        print(\"loading %s...\" % self.partition)\n",
    "        df = self.metadata[\n",
    "            (self.metadata.task == self.task)\n",
    "            & (self.metadata.partition == self.partition)\n",
    "        ]\n",
    "        if self.augment:\n",
    "            path = config(\"augmented_image_path\")\n",
    "        else:\n",
    "            path = config(\"image_path\")\n",
    "\n",
    "        X, y = [], []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row[\"numeric_label\"]\n",
    "            image = imread(os.path.join(path, row[\"filename\"]))\n",
    "            X.append(image)\n",
    "            y.append(row[\"numeric_label\"])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def get_semantic_label(self, numeric_label):\n",
    "        \"\"\"Return the string representation of the numeric class label.\n",
    "\n",
    "        (e.g., the numeric label 1 maps to the semantic label 'hofburg_imperial_palace').\n",
    "        \"\"\"\n",
    "        return self.semantic_labels[numeric_label]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.set_printoptions(precision=3)\n",
    "    tr, va, te, standardizer = get_train_val_test_datasets(\n",
    "        task=\"target\", augment=False)\n",
    "    print(\"Train:\\t\", len(tr.X))\n",
    "    print(\"Val:\\t\", len(va.X))\n",
    "    print(\"Test:\\t\", len(te.X))\n",
    "    print(\"Mean:\", standardizer.image_mean)\n",
    "    print(\"Std: \", standardizer.image_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Landmarks Dataset\n",
    "    Class wrapper for interfacing with the dataset of landmark images\n",
    "    Usage: python dataset.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio.v3 import imread\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import config\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def get_train_val_test_loaders(task, batch_size, **kwargs):\n",
    "    \"\"\"Return DataLoaders for train, val and test splits.\n",
    "\n",
    "    Any keyword arguments are forwarded to the LandmarksDataset constructor.\n",
    "    \"\"\"\n",
    "    tr, va, te, _ = get_train_val_test_datasets(task, **kwargs)\n",
    "\n",
    "    tr_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va, batch_size=batch_size, shuffle=False)\n",
    "    te_loader = DataLoader(te, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return tr_loader, va_loader, te_loader, tr.get_semantic_label\n",
    "\n",
    "\n",
    "def get_challenge(task, batch_size, **kwargs):\n",
    "    \"\"\"Return DataLoader for challenge dataset.\n",
    "\n",
    "    Any keyword arguments are forwarded to the LandmarksDataset constructor.\n",
    "    \"\"\"\n",
    "    tr = LandmarksDataset(\"train\", task, **kwargs)\n",
    "    ch = LandmarksDataset(\"challenge\", task, **kwargs)\n",
    "\n",
    "    standardizer = ImageStandardizer()\n",
    "    standardizer.fit(tr.X)\n",
    "    tr.X = standardizer.transform(tr.X)\n",
    "    ch.X = standardizer.transform(ch.X)\n",
    "\n",
    "    tr.X = tr.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    ch.X = ch.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    ch_loader = DataLoader(ch, batch_size=batch_size, shuffle=False)\n",
    "    return ch_loader, tr.get_semantic_label\n",
    "\n",
    "\n",
    "def get_train_val_test_datasets(task=\"default\", **kwargs):\n",
    "    \"\"\"Return LandmarksDatasets and image standardizer.\n",
    "\n",
    "    Image standardizer should be fit to train data and applied to all splits.\n",
    "    \"\"\"\n",
    "    tr = LandmarksDataset(\"train\", task, **kwargs)\n",
    "    va = LandmarksDataset(\"val\", task, **kwargs)\n",
    "    te = LandmarksDataset(\"test\", task, **kwargs)\n",
    "\n",
    "    # Resize\n",
    "    # You may want to experiment with resizing images to be smaller\n",
    "    # for the challenge portion. How might this affect your training?\n",
    "    # tr.X = resize(tr.X)\n",
    "    # va.X = resize(va.X)\n",
    "    # te.X = resize(te.X)\n",
    "\n",
    "    # Standardize\n",
    "    standardizer = ImageStandardizer()\n",
    "    standardizer.fit(tr.X)\n",
    "    tr.X = standardizer.transform(tr.X)\n",
    "    va.X = standardizer.transform(va.X)\n",
    "    te.X = standardizer.transform(te.X)\n",
    "\n",
    "    # Transpose the dimensions from (N,H,W,C) to (N,C,H,W)\n",
    "    tr.X = tr.X.transpose(0, 3, 1, 2)\n",
    "    va.X = va.X.transpose(0, 3, 1, 2)\n",
    "    te.X = te.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return tr, va, te, standardizer\n",
    "\n",
    "\n",
    "def resize(X):\n",
    "    \"\"\"Resize the data partition X to the size specified in the config file.\n",
    "\n",
    "    Use bicubic interpolation for resizing.\n",
    "\n",
    "    Returns:\n",
    "        the resized images as a numpy array.\n",
    "    \"\"\"\n",
    "    image_dim = config(\"image_dim\")\n",
    "    image_size = (image_dim, image_dim)\n",
    "    resized = []\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = Image.fromarray(X[i]).resize(image_size, resample=2)\n",
    "        resized.append(xi)\n",
    "    resized = [np.asarray(im) for im in resized]\n",
    "    resized = np.array(resized)\n",
    "\n",
    "    return resized\n",
    "\n",
    "\n",
    "class ImageStandardizer(object):\n",
    "    \"\"\"Standardize a batch of images to mean 0 and variance 1.\n",
    "\n",
    "    The standardization should be applied separately to each channel.\n",
    "    The mean and standard deviation parameters are computed in `fit(X)` and\n",
    "    applied using `transform(X)`.\n",
    "\n",
    "    X has shape (N, image_height, image_width, color_channel), where N is\n",
    "    the number of images in the set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mean and standard deviations to None.\"\"\"\n",
    "        super().__init__()\n",
    "        self.image_mean = None\n",
    "        self.image_std = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Calculate per-channel mean and standard deviation from dataset X.\n",
    "        Hint: you may find the axis parameter helpful\"\"\"\n",
    "        self.image_mean = np.mean(X, axis=(0,1,2))\n",
    "        self.image_std = np.std(X, axis=(0,1,2))\n",
    "            \n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Return standardized dataset given dataset X.\"\"\"\n",
    "        X1 = X - self.image_mean\n",
    "        X2 = X1 / self.image_std\n",
    "        return X2\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "class LandmarksDataset(Dataset):\n",
    "    \"\"\"Dataset class for landmark images.\"\"\"\n",
    "\n",
    "    def __init__(self, partition, task=\"target\", augment=False):\n",
    "        \"\"\"Read in the necessary data from disk.\n",
    "\n",
    "        For parts 2, 3 and data augmentation, `task` should be \"target\".\n",
    "        For source task of part 4, `task` should be \"source\".\n",
    "\n",
    "        For data augmentation, `augment` should be True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if partition not in [\"train\", \"val\", \"test\", \"challenge\"]:\n",
    "            raise ValueError(\"Partition {} does not exist\".format(partition))\n",
    "\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        random.seed(42)\n",
    "        self.partition = partition\n",
    "        self.task = task\n",
    "        self.augment = augment\n",
    "        # Load in all the data we need from disk\n",
    "        if task == \"target\" or task == \"source\":\n",
    "            self.metadata = pd.read_csv(config(\"csv_file\"))\n",
    "        if self.augment:\n",
    "            print(\"Augmented\")\n",
    "            self.metadata = pd.read_csv(config(\"augmented_csv_file\"))\n",
    "        self.X, self.y = self._load_data()\n",
    "\n",
    "        self.semantic_labels = dict(\n",
    "            zip(\n",
    "                self.metadata[self.metadata.task == self.task][\"numeric_label\"],\n",
    "                self.metadata[self.metadata.task == self.task][\"semantic_label\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return size of dataset.\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (image, label) pair at index `idx` of dataset.\"\"\"\n",
    "        return torch.from_numpy(\n",
    "            self.X[idx]).float(), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load a single data partition from file.\"\"\"\n",
    "        print(\"loading %s...\" % self.partition)\n",
    "        df = self.metadata[\n",
    "            (self.metadata.task == self.task)\n",
    "            & (self.metadata.partition == self.partition)\n",
    "        ]\n",
    "        if self.augment:\n",
    "            path = config(\"augmented_image_path\")\n",
    "        else:\n",
    "            path = config(\"image_path\")\n",
    "\n",
    "        X, y = [], []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row[\"numeric_label\"]\n",
    "            image = imread(os.path.join(path, row[\"filename\"]))\n",
    "            X.append(image)\n",
    "            y.append(row[\"numeric_label\"])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def get_semantic_label(self, numeric_label):\n",
    "        \"\"\"Return the string representation of the numeric class label.\n",
    "\n",
    "        (e.g., the numeric label 1 maps to the semantic label 'hofburg_imperial_palace').\n",
    "        \"\"\"\n",
    "        return self.semantic_labels[numeric_label]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.set_printoptions(precision=3)\n",
    "    tr, va, te, standardizer = get_train_val_test_datasets(\n",
    "        task=\"target\", augment=False)\n",
    "    print(\"Train:\\t\", len(tr.X))\n",
    "    print(\"Val:\\t\", len(va.X))\n",
    "    print(\"Test:\\t\", len(te.X))\n",
    "    print(\"Mean:\", standardizer.image_mean)\n",
    "    print(\"Std: \", standardizer.image_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection.py\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "dataset_dir = \"data/challenge_augmented\"\n",
    "\n",
    "image_file_paths = []\n",
    "\n",
    "for file_path in glob.glob(dataset_dir + \"/*.png\"):\n",
    "    image_file_paths.append(file_path)\n",
    "\n",
    "for image_file_path in image_file_paths:\n",
    "\n",
    "    image = cv2.imread(image_file_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray_image, 150, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(image, contours, -1, (128, 128, 128), 1)\n",
    "    cv2.imwrite(image_file_path, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_challenge.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Predict Challenge\n",
    "    Runs the challenge model inference on the test dataset and saves the\n",
    "    predictions to disk\n",
    "    Usage: python predict_challenge.py --uniqname=<uniqname>\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from dataset import get_challenge\n",
    "from model.challenge import Challenge\n",
    "from train_common import *\n",
    "from utils import config\n",
    "\n",
    "import utils\n",
    "from sklearn import metrics\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def predict_challenge(data_loader, model):\n",
    "    \"\"\"\n",
    "    Runs the model inference on the test set and outputs the predictions\n",
    "    \"\"\"\n",
    "    y_score = []\n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        y_score.append(softmax(output.data, dim=1)[:, 1])\n",
    "    return torch.cat(y_score)\n",
    "\n",
    "def main(uniqname):\n",
    "    \"\"\"Train challenge model.\"\"\"\n",
    "    # data loaders\n",
    "    if check_for_augmented_data(\"./data\"):\n",
    "        ch_loader, get_semantic_label = get_challenge(\n",
    "            task=\"target\",\n",
    "            batch_size=config(\"challenge.batch_size\"), augment = True\n",
    "        )\n",
    "    else:\n",
    "        ch_loader, get_semantic_label = get_challenge(\n",
    "            task=\"target\",\n",
    "            batch_size=config(\"challenge.batch_size\"),\n",
    "        )\n",
    "\n",
    "    model = Challenge()\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    model, _, _ = restore_checkpoint(model, config(\"challenge.checkpoint\"))\n",
    "\n",
    "    # Evaluate model\n",
    "    model_pred = predict_challenge(ch_loader, model)\n",
    "    print(\"saving challenge predictions...\\n\")\n",
    "    pd_writer = pd.DataFrame(model_pred, columns=[\"predictions\"])\n",
    "    pd_writer.to_csv(uniqname + \".csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--uniqname\", required=True)\n",
    "    args = parser.parse_args()\n",
    "    main(args.uniqname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng_control.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cnn.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Test CNN\n",
    "    Test our trained CNN from train_cnn.py on the heldout test data.\n",
    "    Load the trained CNN model from a saved checkpoint and evaulates using\n",
    "    accuracy and AUROC metrics.\n",
    "    Usage: python test_cnn.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Print performance metrics for model at specified epoch.\"\"\"\n",
    "    # Data loaders\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"target\",\n",
    "        batch_size=config(\"target.batch_size\"),\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Target()\n",
    "\n",
    "    # define loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading cnn...\")\n",
    "    model, start_epoch, stats = restore_checkpoint(\n",
    "        model, config(\"target.checkpoint\"))\n",
    "\n",
    "    axes = utils.make_training_plot()\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,\n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        include_test=True,\n",
    "        update_plot=False,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_challenge.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Train Challenge\n",
    "    Train a convolutional neural network to classify the heldout images\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python train_challenge.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "# from model.challenge import Challenge\n",
    "from model.challenge import *\n",
    "from train_common import *\n",
    "from challenge_target import *\n",
    "# import challenge_source as chas\n",
    "from model.challenge_source import *\n",
    "from utils import config\n",
    "import utils\n",
    "import copy\n",
    "\n",
    "def freeze_layers(model, size=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (challenge): a model built in challenge.py\n",
    "        size (int, optional): the size of layers to be frozen. Defaults to 0.\n",
    "    Due to the time limit, I only implemented \n",
    "    1. half of the conv layers are freezed: size=1\n",
    "    2. the whole conv layers are freezed: size=2\n",
    "    \"\"\"\n",
    "    if size <= 0:\n",
    "        return\n",
    "    num = size\n",
    "    for param in model.parameters():\n",
    "        if num == 0:\n",
    "            break\n",
    "        param.requires_grad = False\n",
    "        num -= 0.5\n",
    "\n",
    "def train(tr_loader, va_loader, te_loader, model, model_name, num_layers=0):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading challenge target model with type\", num_layers, \"frozen\")\n",
    "    model, start_epoch, stats = restore_checkpoint(\n",
    "        model, model_name\n",
    "    )\n",
    "\n",
    "    axes = utils.make_training_plot(\"Challenge Target Training\")\n",
    "\n",
    "    # Evaluate the randomly initialized model\n",
    "    evaluate_epoch(\n",
    "        axes, tr_loader, va_loader, \n",
    "        te_loader, model, criterion, start_epoch, stats, include_test=True\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: Define patience for early stopping. Replace \"None\" with the patience value.\n",
    "    patience = 12\n",
    "    curr_count_to_patience = 0\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes, tr_loader, va_loader, \n",
    "            te_loader, model, criterion, epoch + 1, stats, include_test=True\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, model_name, stats)\n",
    "\n",
    "        # TODO: Implement early stopping\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        #\n",
    "        epoch += 1\n",
    "    print(\"Finished Training\")\n",
    "    # Save figure and keep plot open\n",
    "    utils.save_challenge_training_plot()\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Data loaders\n",
    "    if check_for_augmented_data(\"./data\"):\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\", batch_size=config(\"challenge.batch_size\"), augment=True\n",
    "        )\n",
    "    else:\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\",\n",
    "            batch_size=config(\"challenge.batch_size\"),\n",
    "        )\n",
    "    \n",
    "    do_transfer_learning = input(\"use transfer learning?y/n\\n\")\n",
    "    if do_transfer_learning == 'n':\n",
    "        use_which_model = input(\"use which model? resnet/original\\n\")\n",
    "        if use_which_model == 'original':\n",
    "            model = Challenge2()\n",
    "        if use_which_model == 'resnet':\n",
    "            model = getResNet18()\n",
    "        \n",
    "        # TODO: Define loss function and optimizer. Replace \"None\" with the appropriate definitions.\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.009)\n",
    "\n",
    "        # Attempts to restore the latest checkpoint if exists\n",
    "        print(\"Loading challenge...\")\n",
    "        model, start_epoch, stats = restore_checkpoint(\n",
    "            model, config(\"challenge.checkpoint\")\n",
    "        )\n",
    "\n",
    "        axes = utils.make_training_plot()\n",
    "\n",
    "        # Evaluate the randomly initialized model\n",
    "        evaluate_epoch(\n",
    "            axes, tr_loader, va_loader, \n",
    "            te_loader, model, criterion, start_epoch, stats, include_test=True\n",
    "        )\n",
    "\n",
    "        # initial val loss for early stopping\n",
    "        global_min_loss = stats[0][1]\n",
    "\n",
    "        # TODO: Define patience for early stopping. Replace \"None\" with the patience value.\n",
    "        patience = 5\n",
    "        curr_count_to_patience = 0\n",
    "\n",
    "        # Loop over the entire dataset multiple times\n",
    "        epoch = start_epoch\n",
    "        while curr_count_to_patience < patience:\n",
    "            # Train model\n",
    "            train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "            # Evaluate model\n",
    "            evaluate_epoch(\n",
    "                axes, tr_loader, va_loader, \n",
    "                te_loader, model, criterion, epoch + 1, stats, include_test=True,\n",
    "            )\n",
    "\n",
    "            # Save model parameters\n",
    "            save_checkpoint(model, epoch + 1, config(\"challenge.checkpoint\"), stats)\n",
    "\n",
    "            # TODO: Implement early stopping\n",
    "            curr_count_to_patience, global_min_loss = early_stopping(\n",
    "                stats, curr_count_to_patience, global_min_loss\n",
    "            )\n",
    "            #\n",
    "            epoch += 1\n",
    "        print(\"Finished Training\")\n",
    "        # Save figure and keep plot open\n",
    "        utils.save_challenge_training_plot()\n",
    "        utils.hold_training_plot()\n",
    "    else:\n",
    "        # freeze_none = getResNet18_target()\n",
    "        freeze_none = getResNet18_source()\n",
    "        print(\"Loading source ...\")\n",
    "        freeze_none, _, _ = restore_checkpoint(\n",
    "            freeze_none, config(\"challenge_source.checkpoint\"), force=True, pretrain=True\n",
    "        )\n",
    "        \n",
    "        freeze_whole = copy.deepcopy(freeze_none)\n",
    "        freeze_layers(freeze_whole, 10)\n",
    "        \n",
    "        # modify the last layer:\n",
    "        num_class = 2\n",
    "        freeze_none.fc = torch.nn.Linear(freeze_none.fc.in_features, num_class)\n",
    "        freeze_whole.fc = torch.nn.Linear(freeze_whole.fc.in_features, num_class)\n",
    "        # freeze_layers.fc = torch.nn.Linear(freeze_layers.fc.in_features, num_classes)\n",
    "\n",
    "        \n",
    "        # train(tr_loader, va_loader, \n",
    "        # te_loader, freeze_none, \"./checkpoints/challenge_target0/\", 0)\n",
    "        train(tr_loader, va_loader, \n",
    "              te_loader, freeze_whole, \"./checkpoints/challenge_target1/\", 1)\n",
    "        \n",
    "    # Model\n",
    "    # model = Challenge()\n",
    "    # resnet_8class, _, _ = restore_checkpoint(\n",
    "    #     freeze_none, \n",
    "    #     config(\"challenge_source.checkpoint\"), force=True, pretrain=True\n",
    "    # )\n",
    "    \n",
    "    # resnet_8class = nn.Sequential(*list(resnet_8class.children())[:-1])\n",
    "    \n",
    "    # add own classifier\n",
    "    # num_class = 2\n",
    "    # classifier = nn.Sequential(\n",
    "    #     nn.Flatten(),\n",
    "    #     nn.Linear(512, num_classes)  # 512 is the number of features in the ResNet's output\n",
    "    # )\n",
    "\n",
    "    # Combine pre-trained ResNet and new classifier\n",
    "    # model = nn.Sequential(resnet_8class, classifier)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_challenge_source.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "Train Source CNN\n",
    "    Train a convolutional neural network to classify images.\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python train_source.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.source import Source\n",
    "from train_common import *\n",
    "import model.challenge_source as mcs\n",
    "from utils import config\n",
    "import utils\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train source model on multiclass data.\"\"\"\n",
    "    # Data loaders\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"source\",\n",
    "        batch_size=config(\"source.batch_size\"),\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = mcs.getResNet18_source()\n",
    "\n",
    "    # TODO: Define loss function and optimizer. Replace \"None\" with the appropriate definitions.\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "    print(\"Number of float-valued parameters:\", count_parameters(model))\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading source...\")\n",
    "    clear_checkpoint(config(\"challenge_source.checkpoint\"))\n",
    "    model, start_epoch, stats = restore_checkpoint(\n",
    "        model, config(\"challenge_source.checkpoint\"))\n",
    "\n",
    "    axes = utils.make_training_plot(\"Challenge Source Training\")\n",
    "\n",
    "    # Evaluate the randomly initialized model\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,                                   \n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        multiclass=True,\n",
    "        include_test=True\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: Define patience for early stopping. Replace \"None\" with the patience value.\n",
    "    patience = 12\n",
    "    curr_count_to_patience = 0\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            multiclass=True,\n",
    "            include_test=True\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, \n",
    "                        config(\"challenge_source.checkpoint\"), stats)\n",
    "\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        epoch += 1\n",
    "\n",
    "    # Save figure and keep plot open\n",
    "    print(\"Finished Training\")\n",
    "    utils.save_source_training_plot()\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cnn.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "\n",
    "Train CNN\n",
    "    Train a convolutional neural network to classify images\n",
    "    Periodically output training information, and save model checkpoints\n",
    "    Usage: python train_cnn.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "\n",
    "import rng_control\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train CNN and show training plots.\"\"\"\n",
    "    # Data loaders\n",
    "    if check_for_augmented_data(\"./data\"): # if \"augmented_landmarks.csv\" exists in the directory, then ask if using it.\n",
    "        # if go into this if statement, shows the user decides to use augmented data\n",
    "        tr_loader, va_loader, te_loader, \n",
    "            _ = get_train_val_test_loaders(\n",
    "            task=\"target\", batch_size=config(\"target.batch_size\"), augment=True\n",
    "        )\n",
    "    else: # do not want to use augmented data\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\",\n",
    "            batch_size=config(\"target.batch_size\"),\n",
    "        )\n",
    "    # Model\n",
    "    model = Target() # target is a class(convolutional neural network)\n",
    "\n",
    "    # TODO: Define loss function and optimizer. Replace \"None\" with the appropriate definitions.\n",
    "    criterion = torch.nn.CrossEntropyLoss() # use cross entropy loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # use adam optimizer\n",
    "\n",
    "    print(\"Number of float-valued parameters:\", \n",
    "            count_parameters(model)) # output number of learnable parameters in the model\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading cnn...\")\n",
    "    \n",
    "    # add clear_checkpoint for 1.f.iii >>>\n",
    "    clear_checkpoint(config(\"target.checkpoint\")) # remove all the original checkpoint\n",
    "    # <<<\n",
    "    \n",
    "    model, start_epoch, stats = restore_checkpoint(\n",
    "        model, config(\"target.checkpoint\")) # restore model from checkpoint if exists\n",
    "\n",
    "    axes = utils.make_training_plot()\n",
    "\n",
    "    # Evaluate the randomly initialized model\n",
    "    evaluate_epoch(\n",
    "        axes, tr_loader, va_loader, te_loader, model, criterion, start_epoch, stats\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: Define patience for early stopping. Replace \"None\" with the patience value.\n",
    "    patience = 5\n",
    "    curr_count_to_patience = 0\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            include_test=False,\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, config(\"target.checkpoint\"), stats)\n",
    "\n",
    "        # update early stopping parameters\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "\n",
    "        epoch += 1\n",
    "    print(\"Finished Training\")\n",
    "    # Save figure and keep plot open\n",
    "    utils.save_cnn_training_plot()\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_common.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023  - Project 2\n",
    "\n",
    "Helper file for common training functions.\n",
    "\"\"\"\n",
    "\n",
    "from utils import config\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn import metrics\n",
    "import utils\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count number of learnable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, epoch, checkpoint_dir, stats):\n",
    "    \"\"\"Save a checkpoint file to `checkpoint_dir`.\n",
    "    We save the model parameters after each epoch. The periodic saving of model parameters is called\n",
    "    checkpointing. Checking is an important technique for training large models in machine learning;\n",
    "    if a hardware failure occurs due to a power outage or our code fails for whatever reason, we\n",
    "    don't want to lose all of our progress.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"stats\": stats,\n",
    "    }\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir, \"epoch={}.checkpoint.pth.tar\".format(epoch))\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def check_for_augmented_data(data_dir):\n",
    "    \"\"\"Ask to use augmented data if `augmented_landmarks.csv` exists in the data directory.\"\"\"\n",
    "    if \"augmented_landmarks.csv\" in os.listdir(data_dir):\n",
    "        print(\"Augmented data found, would you like to use it? y/n\")\n",
    "        print(\">> \", end=\"\")\n",
    "        rep = str(input())\n",
    "        return rep == \"y\"\n",
    "    return False\n",
    "\n",
    "\n",
    "def restore_checkpoint(model, checkpoint_dir, cuda=False, force=False, pretrain=False):\n",
    "    \"\"\"Restore model from checkpoint if it exists.\n",
    "    Returns the model and the current epoch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cp_files = [\n",
    "            file_\n",
    "            for file_ in os.listdir(checkpoint_dir)\n",
    "            if file_.startswith(\"epoch=\") and file_.endswith(\".checkpoint.pth.tar\")\n",
    "        ]\n",
    "    except FileNotFoundError:\n",
    "        cp_files = None\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not cp_files:\n",
    "        print(\"No saved model parameters found\")\n",
    "        if force:\n",
    "            raise Exception(\"Checkpoint not found\")\n",
    "        else:\n",
    "            return model, 0, []\n",
    "\n",
    "    # Find latest epoch\n",
    "    for i in itertools.count(1):\n",
    "        if \"epoch={}.checkpoint.pth.tar\".format(i) in cp_files:\n",
    "            epoch = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if not force:\n",
    "        print(\n",
    "            \"Which epoch to load from? Choose in range [0, {}].\".format(epoch),\n",
    "            \"Enter 0 to train from scratch.\",\n",
    "        )\n",
    "        print(\">> \", end=\"\")\n",
    "        inp_epoch = int(input())\n",
    "        if inp_epoch not in range(epoch + 1):\n",
    "            raise Exception(\"Invalid epoch number\")\n",
    "        if inp_epoch == 0:\n",
    "            print(\"Checkpoint not loaded\")\n",
    "            clear_checkpoint(checkpoint_dir)\n",
    "            return model, 0, []\n",
    "    else:\n",
    "        print(\"Which epoch to load from? Choose in range [1, {}].\".format(epoch))\n",
    "        inp_epoch = int(input())\n",
    "        if inp_epoch not in range(1, epoch + 1):\n",
    "            raise Exception(\"Invalid epoch number\")\n",
    "\n",
    "    filename = os.path.join(\n",
    "        checkpoint_dir, \"epoch={}.checkpoint.pth.tar\".format(inp_epoch)\n",
    "    )\n",
    "\n",
    "    print(\"Loading from checkpoint {}?\".format(filename))\n",
    "\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
    "        \n",
    "    print(\"the check point is:\\n\", checkpoint)\n",
    "\n",
    "    try:\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        stats = checkpoint[\"stats\"]\n",
    "        if pretrain:\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(\n",
    "            \"=> Successfully restored checkpoint (trained for {} epochs)\".format(\n",
    "                checkpoint[\"epoch\"]\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        print(\"=> Checkpoint not successfully restored\")\n",
    "        raise\n",
    "\n",
    "    return model, inp_epoch, stats\n",
    "\n",
    "\n",
    "def clear_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Remove checkpoints in `checkpoint_dir`.\"\"\"\n",
    "    filelist = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth.tar\")]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(checkpoint_dir, f))\n",
    "\n",
    "    print(\"Checkpoint successfully removed\")\n",
    "\n",
    "\n",
    "def early_stopping(stats, curr_count_to_patience, global_min_loss):\n",
    "    \"\"\"Calculate new patience and validation loss.\n",
    "\n",
    "    Increment curr_patience by one if new loss is not less than global_min_loss\n",
    "    Otherwise, update global_min_loss with the current val loss, and reset curr_count_to_patience to 0\n",
    "\n",
    "    Returns: new values of curr_patience and global_min_loss\n",
    "    \"\"\"\n",
    "    # TODO implement early stopping\n",
    "    if stats[-1][1] >= global_min_loss:\n",
    "        curr_count_to_patience += 1\n",
    "    else:\n",
    "        global_min_loss = stats[-1][1]\n",
    "        curr_count_to_patience = 0\n",
    "    \n",
    "    return curr_count_to_patience, global_min_loss\n",
    "\n",
    "\n",
    "def evaluate_epoch(\n",
    "    axes,\n",
    "    tr_loader,\n",
    "    val_loader,\n",
    "    te_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    epoch,\n",
    "    stats,\n",
    "    include_test=False,\n",
    "    update_plot=True,\n",
    "    multiclass=False,\n",
    "):\n",
    "    \"\"\"Evaluate the `model` on the train and validation set.\"\"\"\n",
    "    \"\"\"\n",
    "    pass the entire validation set (in batches) through the network\n",
    "    and get the model's predictions, and compare these with the true \n",
    "    labels to get and evaluation metric\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_metrics(loader):\n",
    "        y_true, y_pred, y_score = [], [], []\n",
    "        correct, total = 0, 0\n",
    "        running_loss = []\n",
    "        for X, y in loader:\n",
    "            with torch.no_grad():\n",
    "                output = model(X)\n",
    "                predicted = predictions(output.data)\n",
    "                y_true.append(y)\n",
    "                y_pred.append(predicted)\n",
    "                if not multiclass:\n",
    "                    y_score.append(softmax(output.data, dim=1)[:, 1])\n",
    "                else:\n",
    "                    y_score.append(softmax(output.data, dim=1))\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "                running_loss.append(criterion(output, y).item())\n",
    "        y_true = torch.cat(y_true)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "        y_score = torch.cat(y_score)\n",
    "        loss = np.mean(running_loss)\n",
    "        acc = correct / total\n",
    "        if not multiclass:\n",
    "            auroc = metrics.roc_auc_score(y_true, y_score)\n",
    "        else:\n",
    "            auroc = metrics.roc_auc_score(y_true, y_score, multi_class=\"ovo\")\n",
    "        return acc, loss, auroc\n",
    "\n",
    "    train_acc, train_loss, train_auc = _get_metrics(tr_loader)\n",
    "    val_acc, val_loss, val_auc = _get_metrics(val_loader)\n",
    "\n",
    "    stats_at_epoch = [\n",
    "        val_acc,\n",
    "        val_loss,\n",
    "        val_auc,\n",
    "        train_acc,\n",
    "        train_loss,\n",
    "        train_auc,\n",
    "    ]\n",
    "    if include_test:\n",
    "        stats_at_epoch += list(_get_metrics(te_loader))\n",
    "\n",
    "    stats.append(stats_at_epoch)\n",
    "    utils.log_training(epoch, stats)\n",
    "    if update_plot:\n",
    "        utils.update_training_plot(axes, epoch, stats)\n",
    "\n",
    "\n",
    "def train_epoch(data_loader, model, criterion, optimizer):\n",
    "    \"\"\"Train the `model` for one epoch of data from `data_loader`.\n",
    "\n",
    "    Use `optimizer` to optimize the specified `criterion`\n",
    "    Definition: within one epoch, we pass batches of training examples through the network,\n",
    "        use back propagation to compute gradients, and update model weights using the gradients.\n",
    "    \"\"\"\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(X)\n",
    "        loss = criterion(predict, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "def predictions(logits):\n",
    "    \"\"\"Determine predicted class index given a tensor of logits.\n",
    "\n",
    "    Example: Given tensor([[0.2, -0.8], [-0.9, -3.1], [0.5, 2.3]]), return tensor([0, 0, 1])\n",
    "\n",
    "    Returns:\n",
    "        the predicted class output as a PyTorch Tensor\n",
    "        the set of outputs is called logits\n",
    "    \"\"\"\n",
    "    pred = []\n",
    "    for i in range(logits.shape[0]):\n",
    "        ans = np.argmax(logits[i])\n",
    "        pred.append(ans)\n",
    "    return torch.tensor(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_source.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "Train Source CNN\n",
    "    Train a convolutional neural network to classify images.\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python train_source.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.source import Source\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train source model on multiclass data.\"\"\"\n",
    "    # Data loaders\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"source\",\n",
    "        batch_size=config(\"source.batch_size\"),\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Source()\n",
    "\n",
    "    # TODO: Define loss function and optimizer. Replace \"None\" with the appropriate definitions.\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "    print(\"Number of float-valued parameters:\", count_parameters(model))\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading source...\")\n",
    "    clear_checkpoint(config(\"source.checkpoint\"))\n",
    "    model, start_epoch, stats = restore_checkpoint(\n",
    "        model, config(\"source.checkpoint\"))\n",
    "\n",
    "    axes = utils.make_training_plot(\"Source Training\")\n",
    "\n",
    "    # Evaluate the randomly initialized model\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,                                 \n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        multiclass=True,\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: Define patience for early stopping. Replace \"None\" with the patience value.\n",
    "    patience = 10\n",
    "    curr_count_to_patience = 0\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            multiclass=True,\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, config(\"source.checkpoint\"), stats)\n",
    "\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        epoch += 1\n",
    "\n",
    "    # Save figure and keep plot open\n",
    "    print(\"Finished Training\")\n",
    "    utils.save_source_training_plot()\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023 - Project 2\n",
    "Train Target\n",
    "    Train a convolutional neural network to classify images.\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python train_target.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "import copy\n",
    "\n",
    "import rng_control\n",
    "\n",
    "\n",
    "def freeze_layers(model, num_layers=0):\n",
    "    \"\"\"Stop tracking gradients on selected layers.\"\"\"\n",
    "    # if num_layers == 1:\n",
    "    #     model.conv1.requires_grad = False\n",
    "    # if num_layers == 2:\n",
    "    #     model.conv1.requires_grad = False\n",
    "    #     model.conv2.requires_grad = False\n",
    "    # if num_layers == 3:\n",
    "    #     model.conv1.requires_grad = False\n",
    "    #     model.conv2.requires_grad = False\n",
    "    #     model.conv3.requires_grad = False\n",
    "    if num_layers <= 0:\n",
    "        return\n",
    "    new_num_layers = num_layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if new_num_layers == 0:\n",
    "            break\n",
    "        param.requires_grad = False\n",
    "        new_num_layers -= 0.5\n",
    "\n",
    "def train(tr_loader, va_loader, te_loader, model, model_name, num_layers=0):\n",
    "    \"\"\"Train transfer learning model.\"\"\"\n",
    "    # TODO: Define loss function and optimizer. Replace \"None\" with the appropriate definitions.\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Loading target model with\", num_layers, \"layers frozen\")\n",
    "    model, start_epoch, stats = restore_checkpoint(model, model_name)\n",
    "\n",
    "    axes = utils.make_training_plot(\"Target Training\")\n",
    "\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,\n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        include_test=True,\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: Define patience for early stopping. Replace \"None\" with the patience value.\n",
    "    patience = 5\n",
    "    curr_count_to_patience = 0\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            include_test=True,\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, model_name, stats)\n",
    "\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    # Keep plot open\n",
    "    utils.save_tl_training_plot(num_layers)\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train transfer learning model and display training plots.\n",
    "\n",
    "    Train four different models with {0, 1, 2, 3} layers frozen.\n",
    "    \"\"\"\n",
    "    # data loaders\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"target\",\n",
    "        batch_size=config(\"target.batch_size\"),\n",
    "    )\n",
    "\n",
    "    freeze_none = Target() # class:2\n",
    "    print(\"Loading source...\")\n",
    "    freeze_none, _, _ = restore_checkpoint( # get trained parameters here\n",
    "        freeze_none, config(\"source.checkpoint\"), force=True, pretrain=True\n",
    "    )\n",
    "\n",
    "    freeze_one = copy.deepcopy(freeze_none)\n",
    "    freeze_two = copy.deepcopy(freeze_none)\n",
    "    freeze_three = copy.deepcopy(freeze_none)\n",
    "\n",
    "    freeze_layers(freeze_one, 1)\n",
    "    freeze_layers(freeze_two, 2)\n",
    "    freeze_layers(freeze_three, 3)\n",
    "\n",
    "    train(tr_loader, va_loader, te_loader, freeze_none, \"./checkpoints/target0/\", 0)\n",
    "    train(tr_loader, va_loader, te_loader, freeze_one, \"./checkpoints/target1/\", 1)\n",
    "    train(tr_loader, va_loader, te_loader, freeze_two, \"./checkpoints/target2/\", 2)\n",
    "    train(tr_loader, va_loader, te_loader, freeze_three, \"./checkpoints/target3/\", 3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023  - Project 2\n",
    "Utility functions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def config(attr):\n",
    "    \"\"\"\n",
    "    Retrieves the queried attribute value from the config file. Loads the\n",
    "    config file on first call.\n",
    "    \"\"\"\n",
    "    if not hasattr(config, \"config\"): # hasattr: return 1 if config has attribute \"config\", 0 otherwise\n",
    "        with open(\"config.json\") as f:\n",
    "            config.config = eval(f.read())\n",
    "    node = config.config\n",
    "    for part in attr.split(\".\"):\n",
    "        node = node[part]\n",
    "    return node\n",
    "\n",
    "\n",
    "def denormalize_image(image):\n",
    "    \"\"\"Rescale the image's color space from (min, max) to (0, 1)\"\"\"\n",
    "    ptp = np.max(image, axis=(0, 1)) - np.min(image, axis=(0, 1))\n",
    "    return (image - np.min(image, axis=(0, 1))) / ptp\n",
    "\n",
    "\n",
    "def hold_training_plot():\n",
    "    \"\"\"\n",
    "    Keep the program alive to display the training plot\n",
    "    \"\"\"\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def log_training(epoch, stats):\n",
    "    \"\"\"Print the train, validation, test accuracy/loss/auroc.\n",
    "\n",
    "    Each epoch in `stats` should have order\n",
    "        [val_acc, val_loss, val_auc, train_acc, ...]\n",
    "    Test accuracy is optional and will only be logged if stats is length 9.\n",
    "    \"\"\"\n",
    "    splits = [\"Validation\", \"Train\", \"Test\"]\n",
    "    metrics = [\"Accuracy\", \"Loss\", \"AUROC\"]\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    for j, split in enumerate(splits):\n",
    "        for i, metric in enumerate(metrics):\n",
    "            idx = len(metrics) * j + i\n",
    "            if idx >= len(stats[-1]):\n",
    "                continue\n",
    "            print(f\"\\t{split} {metric}:{round(stats[-1][idx],4)}\")\n",
    "\n",
    "\n",
    "def make_training_plot(name=\"CNN Training\"):\n",
    "    \"\"\"Set up an interactive matplotlib graph to log metrics during training.\"\"\"\n",
    "    plt.ion()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    plt.suptitle(name)\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[2].set_xlabel(\"Epoch\")\n",
    "    axes[2].set_ylabel(\"AUROC\")\n",
    "\n",
    "    return axes\n",
    "\n",
    "\n",
    "def update_training_plot(axes, epoch, stats):\n",
    "    \"\"\"Update the training plot with a new data point for loss and accuracy.\"\"\"\n",
    "    splits = [\"Validation\", \"Train\", \"Test\"]\n",
    "    metrics = [\"Accuracy\", \"Loss\", \"AUROC\"]\n",
    "    colors = [\"r\", \"b\", \"g\"]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for j, split in enumerate(splits):\n",
    "            idx = len(metrics) * j + i\n",
    "            if idx >= len(stats[-1]):\n",
    "                continue\n",
    "            # __import__('pdb').set_trace()\n",
    "            axes[i].plot(\n",
    "                range(epoch - len(stats) + 1, epoch + 1),\n",
    "                [stat[idx] for stat in stats],\n",
    "                linestyle=\"--\",\n",
    "                marker=\"o\",\n",
    "                color=colors[j],\n",
    "            )\n",
    "        axes[i].legend(splits[: int(len(stats[-1]) / len(metrics))])\n",
    "    plt.pause(0.00001)\n",
    "\n",
    "\n",
    "def save_cnn_training_plot():\n",
    "    \"\"\"Save the training plot to a file.\"\"\"\n",
    "    plt.savefig(\"cnn_training_plot.png\", dpi=200)\n",
    "\n",
    "\n",
    "def save_tl_training_plot(num_layers):\n",
    "    \"\"\"Save the transfer learning training plot to a file.\"\"\"\n",
    "    if num_layers == 0:\n",
    "        plt.savefig(\"TL_0_layers.png\", dpi=200)\n",
    "    elif num_layers == 1:\n",
    "        plt.savefig(\"TL_1_layers.png\", dpi=200)\n",
    "    elif num_layers == 2:\n",
    "        plt.savefig(\"TL_2_layers.png\", dpi=200)\n",
    "    elif num_layers == 3:\n",
    "        plt.savefig(\"TL_3_layers.png\", dpi=200)\n",
    "\n",
    "\n",
    "def save_source_training_plot():\n",
    "    \"\"\"Save the source learning training plot to a file.\"\"\"\n",
    "    plt.savefig(\"source_training_plot.png\", dpi=200)\n",
    "\n",
    "\n",
    "def save_challenge_training_plot():\n",
    "    \"\"\"Save the challenge learning training plot to a file.\"\"\"\n",
    "    plt.savefig(\"challenge_training_plot.png\", dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_cnn.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023  - Project 2\n",
    "Grad-CAM Visualization\n",
    "    This script generates a heat map on top of the original image for 20 \n",
    "    sample images in the dataset to help visualize what is learned by\n",
    "    the convolutional networks.\n",
    "\n",
    "    Output files will be titled CNN_viz1_<number>.png\n",
    "\n",
    "    Usage: python visualize_cc.py\n",
    "\n",
    "Original credit to:\n",
    "Author:   Kazuto Nakashima\n",
    "URL:      http://kazuto1011.github.io\n",
    "Created:  2017-05-26\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from model.source import Source\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "from collections import OrderedDict\n",
    "from torch.nn import functional as F\n",
    "from imageio.v3 import imread\n",
    "\n",
    "\n",
    "def save_gradcam(gcam, original_image, axarr, i):\n",
    "    cmap = cm.viridis(np.squeeze(gcam.numpy()))[..., :3] * 255.0\n",
    "    raw_image = (\n",
    "        (\n",
    "            (original_image - original_image.min())\n",
    "            / (original_image.max() - original_image.min())\n",
    "        )\n",
    "        * 255\n",
    "    ).astype(\"uint8\")\n",
    "    gcam = (cmap.astype(np.float64) + raw_image.astype(np.float64)) / 2\n",
    "    axarr[1].imshow(np.uint8(gcam))\n",
    "    axarr[1].axis(\"off\")\n",
    "    plt.savefig(\"CNN_viz1_{}.png\".format(i), dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "class _BaseWrapper(object):\n",
    "    \"\"\"\n",
    "    Please modify forward() and backward() according to your task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, fmaps=None):\n",
    "        super(_BaseWrapper, self).__init__()\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.handlers = []  # a set of hook function handlers\n",
    "\n",
    "    def _encode_one_hot(self, ids):\n",
    "        one_hot = torch.zeros_like(self.logits).to(self.device)\n",
    "        one_hot.scatter_(1, ids, 1.0)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Simple classification\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        self.logits = self.model(image)\n",
    "        if type(self.logits) is tuple:\n",
    "            self.logits = self.logits[0]\n",
    "        self.probs = torch.nn.Sigmoid()(self.logits)\n",
    "        return self.probs.sort(dim=1, descending=True)\n",
    "\n",
    "    def backward(self, ids):\n",
    "        \"\"\"\n",
    "        Class-specific backpropagation\n",
    "        Either way works:\n",
    "        1. self.logits.backward(gradient=one_hot, retain_graph=True)\n",
    "        2. (self.logits * one_hot).sum().backward(retain_graph=True)\n",
    "        \"\"\"\n",
    "        one_hot = self._encode_one_hot(ids)\n",
    "        self.logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "    def generate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_hook(self):\n",
    "        \"\"\"\n",
    "        Remove all the forward/backward hook functions\n",
    "        \"\"\"\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "class BackPropagation(_BaseWrapper):\n",
    "    def forward(self, image):\n",
    "        self.image = image\n",
    "        self.image.requires_grad = False\n",
    "        return super(BackPropagation, self).forward(self.image)\n",
    "\n",
    "    def generate(self):\n",
    "        gradient = self.image.grad.clone()\n",
    "        self.image.grad.zero_()\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class GradCAM(_BaseWrapper):\n",
    "    \"\"\"\n",
    "    \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n",
    "    https://arxiv.org/pdf/1610.02391.pdf\n",
    "    Look at Figure 2 on page 4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, candidate_layers=None):\n",
    "        super(GradCAM, self).__init__(model)\n",
    "        self.fmap_pool = OrderedDict()\n",
    "        self.grad_pool = OrderedDict()\n",
    "        self.candidate_layers = candidate_layers  # list\n",
    "\n",
    "        def forward_hook(key):\n",
    "            def forward_hook_(module, input, output):\n",
    "                # Save featuremaps\n",
    "                a = output.detach().cpu()\n",
    "                self.fmap_pool[key] = a\n",
    "                del output\n",
    "                del a\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            return forward_hook_\n",
    "\n",
    "        def backward_hook(key):\n",
    "            def backward_hook_(module, grad_in, grad_out):\n",
    "                # Save the gradients correspond to the featuremaps\n",
    "                a = grad_out[0].detach().cpu()\n",
    "                self.grad_pool[key] = a\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            return backward_hook_\n",
    "\n",
    "        # If any candidates are not specified, the hook is registered to all the layers.\n",
    "        for name, module in self.model.named_modules():\n",
    "            if self.candidate_layers is None or name in self.candidate_layers:\n",
    "                self.handlers.append(\n",
    "                    module.register_forward_hook(forward_hook(name)))\n",
    "                self.handlers.append(\n",
    "                    module.register_full_backward_hook(backward_hook(name))\n",
    "                )\n",
    "\n",
    "    def find(self, pool, target_layer):\n",
    "        if target_layer in pool.keys():\n",
    "            return pool[target_layer]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer name: {}\".format(target_layer))\n",
    "\n",
    "    def _compute_grad_weights(self, grads):\n",
    "        return F.adaptive_avg_pool2d(grads, 1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.image = image\n",
    "        self.image_shape = image.shape[2:]\n",
    "        return super(GradCAM, self).forward(self.image)\n",
    "\n",
    "    def generate(self, target_layer):\n",
    "        fmaps = self.find(self.fmap_pool, target_layer)\n",
    "        grads = self.find(self.grad_pool, target_layer)\n",
    "        weights = self._compute_grad_weights(grads)\n",
    "        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n",
    "        gcam = F.relu(gcam)\n",
    "        gcam = F.interpolate(\n",
    "            gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        B, C, H, W = gcam.shape\n",
    "        gcam = gcam.view(B, -1)\n",
    "        gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "        gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "        gcam = gcam.view(B, C, H, W)\n",
    "        return gcam\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def get_image(img_num):\n",
    "    img_path = \"data/images/\" + img_num + \".png\"\n",
    "    img = imread(img_path)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_input(img_num, axarr):\n",
    "    xi = get_image(img_num)\n",
    "    axarr[0].imshow(utils.denormalize_image(xi))\n",
    "    axarr[0].axis(\"off\")\n",
    "\n",
    "\n",
    "def visualize_layer1_activations(img_num, i, axarr):\n",
    "    xi = get_image(img_num)\n",
    "    xi = xi.transpose(2, 0, 1)\n",
    "    xi = torch.from_numpy(xi).float()\n",
    "    xi = xi.view((1, 3, 64, 64))\n",
    "    bp = BackPropagation(model=model)\n",
    "    gcam = GradCAM(model=model)\n",
    "    target_layer = \"conv1\"\n",
    "    target_class = 1\n",
    "    _ = gcam.forward(xi)\n",
    "    gcam.backward(ids=torch.tensor([[target_class]]).to(device))\n",
    "    regions = gcam.generate(target_layer=target_layer)\n",
    "    activation = regions.detach()\n",
    "    save_gradcam(\n",
    "        np.squeeze(activation),\n",
    "        utils.denormalize_image(np.squeeze(xi.numpy()).transpose(1, 2, 0)),\n",
    "        axarr,\n",
    "        i,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Attempts to restore from checkpoint\n",
    "    print(\"Loading cnn...\")\n",
    "    model = Target()\n",
    "    model, start_epoch, _ = restore_checkpoint(\n",
    "        model, config(\"target.checkpoint\"), force=True\n",
    "    )\n",
    "\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"target\",\n",
    "        batch_size=config(\"target.batch_size\"),\n",
    "    )\n",
    "\n",
    "    # img_list contains the ids for a sample of images from the training set\n",
    "    img_list = ['01105', '00024', '03545', '05934', '03165',\n",
    "                '05354', '01295', '00914', '03035', '07984',\n",
    "                '09125', '05654', '05125', '05174', '04685',\n",
    "                '12235', '00975', '01074', '02995', '01134']\n",
    "    for i, img_num in enumerate(img_list):\n",
    "        plt.clf()\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        visualize_input(img_num, axarr)\n",
    "        visualize_layer1_activations(img_num, i, axarr)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_data.py\n",
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Fall 2023  - Project 2\n",
    "Visualize Landmarks\n",
    "    This will open up a window displaying randomly selected training\n",
    "    images. The label of the image is shown. Click on the figure to\n",
    "    refresh with a set of new images. You can save the images using\n",
    "    the save button. Close the window to break out of the loop.\n",
    "\n",
    "    The success of this script is a good indication that the data flow\n",
    "    part of this project is running smoothly.\n",
    "\n",
    "    Usage: python visualize_data.py\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import platform\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import resize, ImageStandardizer, LandmarksDataset\n",
    "from imageio.v3 import imread\n",
    "from utils import config, denormalize_image\n",
    "\n",
    "OUT_FILENAME = \"visualize_data.png\"\n",
    "\n",
    "training_set = LandmarksDataset(\"train\")\n",
    "training_set.X = resize(training_set.X)\n",
    "standardizer = ImageStandardizer()\n",
    "standardizer.fit(training_set.X)\n",
    "\n",
    "metadata = pd.read_csv(config(\"csv_file\"))\n",
    "metadata = metadata[metadata[\"partition\"] != \"challenge\"].reset_index(drop=True)\n",
    "\n",
    "N = 4\n",
    "fig, axes = plt.subplots(nrows=2, ncols=N, figsize=(2 * N, 2 * 2))\n",
    "\n",
    "pad = 3\n",
    "axes[0, 0].annotate(\n",
    "    \"Original\",\n",
    "    xy=(0, 0.5),\n",
    "    xytext=(-axes[0, 0].yaxis.labelpad - pad, 0),\n",
    "    xycoords=axes[0, 0].yaxis.label,\n",
    "    textcoords=\"offset points\",\n",
    "    size=\"large\",\n",
    "    ha=\"right\",\n",
    "    va=\"center\",\n",
    "    rotation=\"vertical\",\n",
    ")\n",
    "axes[1, 0].annotate(\n",
    "    \"Preprocessed\",\n",
    "    xy=(0, 0.5),\n",
    "    xytext=(-axes[1, 0].yaxis.labelpad - pad, 0),\n",
    "    xycoords=axes[1, 0].yaxis.label,\n",
    "    textcoords=\"offset points\",\n",
    "    size=\"large\",\n",
    "    ha=\"right\",\n",
    "    va=\"center\",\n",
    "    rotation=\"vertical\",\n",
    ")\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def display_imgs():\n",
    "    rand_idx = np.random.choice(np.arange(len(metadata)), size=N, replace=False)\n",
    "    X, y = [], []\n",
    "    for idx in rand_idx:\n",
    "        filename = os.path.join(config(\"image_path\"), \n",
    "                                metadata.loc[idx, \"filename\"])\n",
    "        X.append(imread(filename))\n",
    "        y.append(metadata.loc[idx, \"semantic_label\"])\n",
    "\n",
    "    for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "        axes[0, i].imshow(xi)\n",
    "        axes[0, i].set_title(yi.replace(\"_\", \"\\n\"))\n",
    "\n",
    "    X_ = resize(np.array(X))\n",
    "    X_ = standardizer.transform(X_)\n",
    "    for i, (xi, yi) in enumerate(zip(X_, y)):\n",
    "        axes[1, i].imshow(denormalize_image(xi), interpolation=\"bicubic\")\n",
    "\n",
    "\n",
    "if platform.system() == \"Linux\" and \"DISPLAY\" not in os.environ:\n",
    "    print(\n",
    "        f'No window server found, switching \\\n",
    "            to writing first {N} images to file \"{OUT_FILENAME}\"'\n",
    "    )\n",
    "    display_imgs()\n",
    "    fig.savefig(OUT_FILENAME, bbox_inches=\"tight\")\n",
    "    exit(0)\n",
    "\n",
    "print(\n",
    "    \"I will display some images. \\\n",
    "        Click on the figure to refresh. Close the figure to exit.\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    display_imgs()\n",
    "    plt.draw()\n",
    "    if plt.get_fignums():\n",
    "        print(plt.get_fignums())\n",
    "    else:\n",
    "        print(None)\n",
    "    fig.savefig(OUT_FILENAME, bbox_inches=\"tight\")\n",
    "    if plt.waitforbuttonpress(0) == None:\n",
    "        break\n",
    "\n",
    "print(\"OK, bye!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

<!DOCTYPE html>
<html>
<head>
<title>challenge.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="challenge">Challenge</h1>
<p>In this project, I build a convolutional neural network based on residual neural network 18 from scratch, and applied several methods including data augmentation, regularization, feature selection and transfer learning to test on models and improve the performance. Finally, I choose to use ResNet18, Epoch=56, trained on augmented data, regularization, and transfer learning. The maximum validation AUROC is 0.9826, greatest testing AUROC is 0.9917</p>
<h2 id="model-evaluation">Model Evaluation</h2>
<p>Since this challenge part only grades AUROC score of our predicted model, I choose AUROC as a main criteria of model evaluation. But it does not mean I do not see loss and accuracy in the whole process. In the later part, whenever there is a experiment on certain methods, I will show the result by presenting an image comprised of three criteria: accuracy, loss and AUROC, and report the maximum AUROC score explicitly.</p>
<h2 id="model-architecture">Model architecture</h2>
<p>From the slides, it's shown that ResNet achieved best performance in 2014 ImageNet Challenge. Therefore, I think this is a good model for me to reference.</p>
<p>In the <code>challenge.py</code>, I implemented a model based on ResNet architecture. It has 18 layers in total, while each layer was normally distributed with $\mu=0.0$, $\sigma=\frac{1}{\sqrt{5\times 5\times num_input_channels}}$.</p>
<p>In the transition of convolutional layers and linear layers, I used an average pooling layer. This can let me avoid calculating the image dimension changes in convolutional layers.</p>
<p>The following image shows the result of my first training and validation.</p>
<ul>
<li>Criterion: cross entropy loss</li>
<li>Optimizer: adam</li>
<li>Learning rate: 0.001</li>
<li>Patience: 5</li>
<li>Batch size: 32</li>
</ul>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/1_raw_5_class_2.png" alt="first.png"></p>
<p>From the figure, we know that</p>
<ul>
<li>the greatest validation AUROC is 0.9535, epoch 7</li>
<li>the lowest validation error is 0.3132, epoch 2
This is not a good result, so we need to make more improvements.</li>
</ul>
<p>For comparison, I also implemented the original model that we used in the previous part.
The result is as follows:
<img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/1_original_5_2.png" alt="first_with_original_architecture"></p>
<p>We can figure out that the ResNet18 has better performance, but it needs a lot of time to train. In the later sections, I will focus on the original model, but when some methods has improvements, I will apply them into resNet.</p>
<h2 id="regularization-weight-decay-dropout-etc">Regularization (Weight decay, dropout, etc.)</h2>
<p>In this section, I will apply some regularization methods. Regularization can be known as a way to adding additional constraints to the optimization problem in order to prevent overfitting. From the above raw test, we can intuitively sense that the model receives lots of noises in the process, and the model overfit when epoch is very big. So, it's reasonable to apply regularization in mitigate the effect of overfitting.</p>
<h3 id="dropout">Dropout</h3>
<p>First, we can employ dropout. I first by applying each dropout after each convolutional layers.</p>
<p>And the result is as follows:</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/2_originalz_5_dropout.png" alt="2_original_5_dropout"></p>
<p>The maximum AUROC is 0.8281, this is not a good performance, so we need to continue improve it.</p>
<p>However, since our original model has better performance than the most original model. we will keep this methods.</p>
<h3 id="weight-decay">Weight Decay</h3>
<p>As shown in the following code, we can add weight decay to mitigate the effect of overfitting.</p>
<pre class="hljs"><code><div>optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>, weight_decay=<span class="hljs-number">0.01</span>)
</div></code></pre>
<p>This principle behind it is quite like what we use L2 norm regularization in support vector machine, and the value we input means the importance of this L2 norm.</p>
<p>After applying this method, we can get the following result.</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/2_original_5_dropout_weightdecay.png" alt="2_original_5_dropout_weightdecay"></p>
<p>The maximum test AUROC is 0.8361, which is better than last trial. So I decide to keep it.</p>
<h3 id="batch-normalization">Batch Normalization</h3>
<p>Given the improvements brought by the above two methods, we are sure that regularization can indeed help the model to increase performance.</p>
<p>So, now I decided to apply batchNormal to this model. Batch Normalization normalizes the inputs of each layer in a mini-batch, which helps stabilize and speed up the training process. Here's the performance after I implement the Batch Normalization to the model.</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/2_original_5_dropout_weightdecay.png" alt="2_original_5_dr_wei_bat"></p>
<p>The maximum AUROC is 0.9021! It's a great improvement!</p>
<p>After implementing dropout, weight decay and batch normalization, the final code is shown as below.</p>
<pre class="hljs"><code><div>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()
        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-number">2</span>)
        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">16</span>) 
        self.conv2 = nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">64</span>, kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-number">2</span>)
        self.bn2 = nn.BatchNorm2d(<span class="hljs-number">64</span>)  
        self.conv3 = nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">8</span>, kernel_size=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), stride=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), padding=<span class="hljs-number">2</span>)
        self.bn3 = nn.BatchNorm2d(<span class="hljs-number">8</span>)   
        self.dropout1 = nn.Dropout(p=<span class="hljs-number">0.1</span>)
        self.dropout2 = nn.Dropout(p=<span class="hljs-number">0.2</span>)
        self.dropout3 = nn.Dropout(p=<span class="hljs-number">0.3</span>)
        self.pool = nn.MaxPool2d(kernel_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))
        self.fc1 = nn.Linear(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>)
        self.init_weights()
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        N, C, H, W = x.shape

        x = F.relu(self.bn1(self.conv1(x)))
        x = self.dropout1(x)
        x = self.pool(x)
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.dropout2(x)
        x = self.pool(x)
        x = F.relu(self.bn3(self.conv3(x)))
        x = x.reshape(N, <span class="hljs-number">32</span>)
        x = self.fc1(x)
        
        <span class="hljs-keyword">return</span> x
</div></code></pre>
<p>And we can apply some similar strategies to ResNet18 model.</p>
<h2 id="transfer-learning">Transfer Learning</h2>
<p>The transfer learning is a good way to let our current model receive more information. Since our original dataset has 8 classes in total, we can train the model on 8 classes first, then transfer the learned parameters and freeze them when training new models on dataset of 2 classes. In this way, we can take advantages of the previous trained model and get better performance.</p>
<p>Since we've already known the power of transfer learning in previous problems, in this part, I will only apply the code on the ResNet18 model.</p>
<p>From section 1, I've already presented my handwritten code of ResNet18. In order to do transfer learning on this model, I need to set the <code>num_classes</code> equal to 8 first, and then store the code into <code>p2/model/challenge_source.py</code>. And I also add <code>train_challenge_source.py</code> to train <code>challenge_source.py</code> directly. Finally, in the <code>train_challenge.py</code>, I add an <code>if</code> statement, asking if user would like to use transfer learning:</p>
<pre class="hljs"><code><div>do_transfer_learning = input(<span class="hljs-string">"use transfer learning?y/n\n"</span>)
</div></code></pre>
<p>If the user replies <code>y</code>, then this function will direct to transfer learning part, which has the code as follows:</p>
<pre class="hljs"><code><div>        freeze_none = getResNet18_source()
        print(<span class="hljs-string">"Loading source ..."</span>)
        freeze_none, _, _ = restore_checkpoint(
            freeze_none, config(<span class="hljs-string">"challenge_source.checkpoint"</span>), force=<span class="hljs-literal">True</span>, pretrain=<span class="hljs-literal">True</span>
        )
        
        
        freeze_whole = copy.deepcopy(freeze_none)
        freeze_layers(freeze_whole, <span class="hljs-number">10</span>)
        
        <span class="hljs-comment"># modify the last layer:</span>
        num_class = <span class="hljs-number">2</span>
        freeze_none.fc = torch.nn.Linear(freeze_none.fc.in_features, num_class)
        freeze_whole.fc = torch.nn.Linear(freeze_whole.fc.in_features, num_class)

        
        train(tr_loader, va_loader, te_loader, freeze_none, <span class="hljs-string">"./checkpoints/challenge_target0/"</span>, <span class="hljs-number">0</span>)
        train(tr_loader, va_loader, te_loader, freeze_whole, <span class="hljs-string">"./checkpoints/challenge_target1/"</span>, <span class="hljs-number">1</span>)
</div></code></pre>
<p>Now, I'd like to briefly discuss the meaning of the above code.</p>
<ul>
<li>In <code>restore_checkpoint</code>, we can restore the checkpoint we learned from <code>train_challenge_source.py</code>.</li>
<li>In <code>freeze_layers</code> function, we can specify how many layers to freeze.</li>
<li>In <code>freeze_whole.fc = torch.nn.Linear(freeze_whole.fc.in_features, num_class)</code>, we modify the final layer (the linear layer) to have the output of 2 since we have 2 classes.</li>
</ul>
<p>After training all the above steps, we can get the following results:</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/3_resnet_transfer_0.png" alt="3_resnet_transfer_0">
<img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/3_resnet_transfer_10.png" alt="3_resnet_transfer_10"></p>
<p>The maximum AUROC is 0.994 and 0,9917, respectively. This is a huge improvement!!!</p>
<h2 id="data-augment">Data Augment</h2>
<p>like what we have done in previous part, we can apply data augmentation to our dataset so that our model can receive more information.</p>
<p>In this section, I decided to use <code>torchvision.transform</code>. The following are code implementations.</p>
<h3 id="rotate">Rotate</h3>
<p>Instead of using fixed rotation like what we have done in previous part, I used random rotation so that the image can be more representative.</p>
<pre class="hljs"><code><div>random_rotation = transforms.RandomRotation(degrees=(<span class="hljs-number">-20</span>, <span class="hljs-number">20</span>))
</div></code></pre>
<h3 id="clip-flip">Clip, Flip</h3>
<p>Clipping and flipping should not change the types of images, this is one of the most popular data augmentation methods.</p>
<pre class="hljs"><code><div>randomVerticalFlip = transforms.RandomVerticalFlip()
random_horizontal_flip = transforms.RandomHorizontalFlip()
random_resized_crop = transforms.RandomResizedCrop(
    (<span class="hljs-number">64</span>, <span class="hljs-number">64</span>), scale=(<span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>), resized=(<span class="hljs-number">0.5</span>, <span class="hljs-number">2</span>))
</div></code></pre>
<h3 id="color-changing">color changing</h3>
<p>We can also change the color of the images. It contain 4 aspects: Brightness, Contrast, Saturation and Hue.</p>
<pre class="hljs"><code><div>torchvision.transforms.ColorJitter(
    brightness=<span class="hljs-number">0.5</span>, contrast=<span class="hljs-number">0.5</span>, saturation=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>
)
</div></code></pre>
<p>Finally, we can add all of the above codes together. The final code is like this:</p>
<pre class="hljs"><code><div>transform_shape = tortransforms.Compose([
    tortransformers.RandomRotation(degrees=(<span class="hljs-number">-20</span>, <span class="hljs-number">20</span>)),
    tortransforms.RandomResizedCrop(
        (<span class="hljs-number">64</span>, <span class="hljs-number">64</span>), scale=(<span class="hljs-number">0.3</span>, <span class="hljs-number">1</span>), retio=(<span class="hljs-number">0.5</span>, <span class="hljs-number">2</span>)),
    tortransforms.RandomHorizontalFlip(),
    tortransforms.ToTensor(),
])

transformer_color = tortransformers.ColorJitter(
  brightnenss=<span class="hljs-number">0.5</span>, contrast=<span class="hljs-number">0.5</span>, saturation=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>
)
</div></code></pre>
<p>Due to the time limit, I will only test this method on our base model.</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/4_original_5_aug.png" alt=""></p>
<p>It turns out that the performance of the model does not have much improvement. But it's still worth a try.</p>
<h2 id="feature-selection">Feature Selection</h2>
<p>In this section, I want to use feature selection to highlight some features in the image so that our model can pay more attention to them. I decided to use edge detection. It's an important feature in image because it tells the shape of a building and is always the boundary of this building. With the help of this, we can eliminate the effect of noisy in the image and make our model concentrate on the shape of a building.</p>
<p>Given that the sky is usually white and the building is usually black (gray) in image, I decide to add gray color to the detected edge.</p>
<p>The code is as follows:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> glob

dataset_dir = <span class="hljs-string">"data/image_backup"</span>

image_file_paths = []

<span class="hljs-keyword">for</span> file_path <span class="hljs-keyword">in</span> glob.glob(dataset_dir + <span class="hljs-string">"/*.png"</span>):
    image_file_paths.append(file_path)

<span class="hljs-keyword">for</span> image_file_path <span class="hljs-keyword">in</span> image_file_paths:

    image = cv2.imread(image_file_path)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(gray_image, <span class="hljs-number">150</span>, <span class="hljs-number">255</span>, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(image, contours, <span class="hljs-number">-1</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>), <span class="hljs-number">1</span>)
    cv2.imwrite(image_file_path, image)
</div></code></pre>
<p>In the above code, I employ some functions in opencv to help me detect the edge of some images. When the edges of a image is detected, the program will add a gray contour of 1 pixel size to it. This actually serves as a way to highlight these features.</p>
<p>One example is like this:</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/edge_detect.png" alt="">
From the image, we can notice that the building and window has gray edges around it.</p>
<p>But sometimes, this implementation may not be a good idea, because this edge detection method sometime has errors.</p>
<p>For example, we can see this picture:</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/edge_detect2.png" alt=""></p>
<p>The sky has some gray line, which is an error that shouldn't happen.</p>
<p>The code is as follows:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> glob

dataset_dir = <span class="hljs-string">"data/image_backup"</span>

image_file_paths = []

<span class="hljs-keyword">for</span> file_path <span class="hljs-keyword">in</span> glob.glob(dataset_dir + <span class="hljs-string">"/*.png"</span>):
    image_file_paths.append(file_path)

<span class="hljs-keyword">for</span> image_file_path <span class="hljs-keyword">in</span> image_file_paths:

    image = cv2.imread(image_file_path)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(gray_image, <span class="hljs-number">150</span>, <span class="hljs-number">255</span>, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(image, contours, <span class="hljs-number">-1</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>), <span class="hljs-number">1</span>)
    cv2.imwrite(image_file_path, image)

</div></code></pre>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/5_original_5_aug_edge.png" alt=""></p>
<p>The maximum AUROC is 0.8406. It's a good number. But given the deviation in prediction, I think this method may introduce too much noise into the dataset.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Based on the above analysis, I'd like to apply data augmentation, transfer learning and regularization to my handwritten resnet18.</p>
<p>The source training result is:</p>
<p><img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/6_resnet_12_source_aug.png" alt="">
The maximum epoch is 56: 0.9826</p>
<p>The final performance of my target model is:
<img src="file:///Users/wdli/Desktop/445/project/p2/challenge_result/7_resnet.png" alt="">
The best AUROC score is: 0.9917</p>

</body>
</html>
